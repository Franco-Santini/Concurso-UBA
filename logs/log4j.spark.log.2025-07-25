25/07/25 09:41:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/25 09:41:19 INFO SparkContext: Running Spark version 2.4.3
25/07/25 09:41:19 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
25/07/25 09:41:19 INFO SparkContext: Submitted application: sparklyr
25/07/25 09:41:19 INFO SecurityManager: Changing view acls to: Usuario
25/07/25 09:41:19 INFO SecurityManager: Changing modify acls to: Usuario
25/07/25 09:41:19 INFO SecurityManager: Changing view acls groups to: 
25/07/25 09:41:19 INFO SecurityManager: Changing modify acls groups to: 
25/07/25 09:41:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Usuario); groups with view permissions: Set(); users  with modify permissions: Set(Usuario); groups with modify permissions: Set()
25/07/25 09:41:19 INFO Utils: Successfully started service 'sparkDriver' on port 51711.
25/07/25 09:41:19 INFO SparkEnv: Registering MapOutputTracker
25/07/25 09:41:19 INFO SparkEnv: Registering BlockManagerMaster
25/07/25 09:41:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/07/25 09:41:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/07/25 09:41:19 INFO DiskBlockManager: Created local directory at C:\Users\Usuario\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\blockmgr-8f464fc0-c44e-4571-826b-07970f2d6e48
25/07/25 09:41:19 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
25/07/25 09:41:19 INFO SparkEnv: Registering OutputCommitCoordinator
25/07/25 09:41:19 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/Usuario/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
25/07/25 09:41:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/07/25 09:41:20 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
25/07/25 09:41:20 INFO SparkContext: Added JAR file:/C:/Users/Usuario/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:51711/jars/sparklyr-2.4-2.11.jar with timestamp 1753447280266
25/07/25 09:41:20 INFO Executor: Starting executor ID driver on host localhost
25/07/25 09:41:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51732.
25/07/25 09:41:20 INFO NettyBlockTransferService: Server created on 127.0.0.1:51732
25/07/25 09:41:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/07/25 09:41:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 51732, None)
25/07/25 09:41:20 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:51732 with 912.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 51732, None)
25/07/25 09:41:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 51732, None)
25/07/25 09:41:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 51732, None)
25/07/25 09:41:21 INFO SharedState: loading hive config file: file:/C:/Users/Usuario/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/conf/hive-site.xml
25/07/25 09:41:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/Usuario/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/Usuario/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive').
25/07/25 09:41:21 INFO SharedState: Warehouse path is 'C:/Users/Usuario/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive'.
25/07/25 09:41:22 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
25/07/25 09:41:26 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
25/07/25 09:41:28 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
25/07/25 09:41:28 INFO ObjectStore: ObjectStore, initialize called
25/07/25 09:41:28 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
25/07/25 09:41:28 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
25/07/25 09:41:30 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
25/07/25 09:41:31 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
25/07/25 09:41:31 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
25/07/25 09:41:32 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
25/07/25 09:41:32 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
25/07/25 09:41:32 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
25/07/25 09:41:32 INFO ObjectStore: Initialized ObjectStore
25/07/25 09:41:32 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
25/07/25 09:41:32 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
25/07/25 09:41:33 INFO HiveMetaStore: Added admin role in metastore
25/07/25 09:41:33 INFO HiveMetaStore: Added public role in metastore
25/07/25 09:41:33 INFO HiveMetaStore: No user is added in admin role, since config is empty
25/07/25 09:41:33 INFO HiveMetaStore: 0: get_all_databases
25/07/25 09:41:33 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_all_databases	
25/07/25 09:41:33 INFO HiveMetaStore: 0: get_functions: db=default pat=*
25/07/25 09:41:33 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
25/07/25 09:41:33 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MResourceUri" es "embedded-only" asi que no tiene su propia tabla.
25/07/25 09:41:33 INFO SessionState: Created local directory: C:/Users/Usuario/AppData/Local/Temp/5153d856-3564-4ad8-ab54-189b3fe1888a_resources
25/07/25 09:41:33 INFO SessionState: Created HDFS directory: C:/Users/Usuario/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/Usuario/5153d856-3564-4ad8-ab54-189b3fe1888a
25/07/25 09:41:33 INFO SessionState: Created local directory: C:/Users/Usuario/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/5153d856-3564-4ad8-ab54-189b3fe1888a
25/07/25 09:41:33 INFO SessionState: Created HDFS directory: C:/Users/Usuario/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/Usuario/5153d856-3564-4ad8-ab54-189b3fe1888a/_tmp_space.db
25/07/25 09:41:33 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is C:/Users/Usuario/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive
25/07/25 09:41:33 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:41:33 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:41:33 INFO HiveMetaStore: 0: get_database: global_temp
25/07/25 09:41:33 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: global_temp	
25/07/25 09:41:33 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
25/07/25 09:41:33 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:41:33 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:41:33 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:41:33 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:41:33 INFO HiveMetaStore: 0: get_tables: db=default pat=*
25/07/25 09:41:33 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
25/07/25 09:41:34 INFO CodeGenerator: Code generated in 265.7026 ms
25/07/25 09:41:35 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:41:35 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:41:35 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:41:35 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:41:35 INFO HiveMetaStore: 0: get_tables: db=default pat=*
25/07/25 09:41:35 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
25/07/25 09:41:38 INFO ContextCleaner: Cleaned accumulator 0
25/07/25 09:41:38 INFO ContextCleaner: Cleaned accumulator 1
25/07/25 09:42:51 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:42:51 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:42:51 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:42:51 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:42:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
25/07/25 09:42:51 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
25/07/25 09:42:51 INFO CodeGenerator: Code generated in 12.3709 ms
25/07/25 09:42:51 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:42:51 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:42:51 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:42:51 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:42:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
25/07/25 09:42:51 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
25/07/25 09:42:51 INFO FileSourceStrategy: Pruning directories with: 
25/07/25 09:42:51 INFO FileSourceStrategy: Post-Scan Filters: 
25/07/25 09:42:51 INFO FileSourceStrategy: Output Data Schema: struct<client_id: int, phone_number: string, email_address: string, city: string, state: string ... 9 more fields>
25/07/25 09:42:51 INFO FileSourceScanExec: Pushed Filters: 
25/07/25 09:42:51 INFO ContextCleaner: Cleaned accumulator 3
25/07/25 09:42:51 INFO ContextCleaner: Cleaned accumulator 2
25/07/25 09:42:52 INFO CodeGenerator: Code generated in 26.5733 ms
25/07/25 09:42:52 INFO CodeGenerator: Code generated in 18.5946 ms
25/07/25 09:42:52 INFO CodeGenerator: Code generated in 11.0479 ms
25/07/25 09:42:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 286.0 KB, free 912.0 MB)
25/07/25 09:42:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.2 KB, free 912.0 MB)
25/07/25 09:42:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:51732 (size: 24.2 KB, free: 912.3 MB)
25/07/25 09:42:52 INFO SparkContext: Created broadcast 0 from sql at <unknown>:0
25/07/25 09:42:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 19166895 bytes, open cost is considered as scanning 4194304 bytes.
25/07/25 09:42:53 INFO SparkContext: Starting job: sql at <unknown>:0
25/07/25 09:42:53 INFO DAGScheduler: Registering RDD 7 (sql at <unknown>:0)
25/07/25 09:42:53 INFO DAGScheduler: Got job 0 (sql at <unknown>:0) with 1 output partitions
25/07/25 09:42:53 INFO DAGScheduler: Final stage: ResultStage 1 (sql at <unknown>:0)
25/07/25 09:42:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
25/07/25 09:42:53 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
25/07/25 09:42:53 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[7] at sql at <unknown>:0), which has no missing parents
25/07/25 09:42:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 25.3 KB, free 912.0 MB)
25/07/25 09:42:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.5 KB, free 912.0 MB)
25/07/25 09:42:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:51732 (size: 11.5 KB, free: 912.3 MB)
25/07/25 09:42:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
25/07/25 09:42:53 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[7] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
25/07/25 09:42:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
25/07/25 09:42:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8340 bytes)
25/07/25 09:42:53 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 8340 bytes)
25/07/25 09:42:53 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 8340 bytes)
25/07/25 09:42:53 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 8340 bytes)
25/07/25 09:42:53 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
25/07/25 09:42:53 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
25/07/25 09:42:53 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
25/07/25 09:42:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/07/25 09:42:53 INFO Executor: Fetching spark://127.0.0.1:51711/jars/sparklyr-2.4-2.11.jar with timestamp 1753447280266
25/07/25 09:42:53 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:51711 after 37 ms (0 ms spent in bootstraps)
25/07/25 09:42:53 INFO Utils: Fetching spark://127.0.0.1:51711/jars/sparklyr-2.4-2.11.jar to C:\Users\Usuario\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e0d71c16-2d49-4ccc-b1ff-f55871ba6a18\userFiles-118793d3-6e19-4d6a-bc0a-c3d246a45ff4\fetchFileTemp7925330762686571640.tmp
25/07/25 09:42:53 INFO Executor: Adding file:/C:/Users/Usuario/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-e0d71c16-2d49-4ccc-b1ff-f55871ba6a18/userFiles-118793d3-6e19-4d6a-bc0a-c3d246a45ff4/sparklyr-2.4-2.11.jar to class loader
25/07/25 09:42:53 INFO FileScanRDD: Reading File path: file:///C:/Users/Usuario/Desktop/Concurso-UBA/Datos/eci_customer_data.csv, range: 19166895-38333790, partition values: [empty row]
25/07/25 09:42:53 INFO FileScanRDD: Reading File path: file:///C:/Users/Usuario/Desktop/Concurso-UBA/Datos/eci_customer_data.csv, range: 0-19166895, partition values: [empty row]
25/07/25 09:42:53 INFO FileScanRDD: Reading File path: file:///C:/Users/Usuario/Desktop/Concurso-UBA/Datos/eci_customer_data.csv, range: 57500685-72473278, partition values: [empty row]
25/07/25 09:42:53 INFO FileScanRDD: Reading File path: file:///C:/Users/Usuario/Desktop/Concurso-UBA/Datos/eci_customer_data.csv, range: 38333790-57500685, partition values: [empty row]
25/07/25 09:42:53 INFO CodeGenerator: Code generated in 41.0338 ms
25/07/25 09:42:58 INFO MemoryStore: Block rdd_2_3 stored as values in memory (estimated size 10.1 MB, free 901.8 MB)
25/07/25 09:42:58 INFO BlockManagerInfo: Added rdd_2_3 in memory on 127.0.0.1:51732 (size: 10.1 MB, free: 902.1 MB)
25/07/25 09:42:58 INFO CodeGenerator: Code generated in 6.3785 ms
25/07/25 09:42:58 INFO CodeGenerator: Code generated in 70.3616 ms
25/07/25 09:42:58 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1957 bytes result sent to driver
25/07/25 09:42:59 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 5673 ms on localhost (executor driver) (1/4)
25/07/25 09:42:59 INFO MemoryStore: Block rdd_2_0 stored as values in memory (estimated size 13.1 MB, free 888.7 MB)
25/07/25 09:42:59 INFO BlockManagerInfo: Added rdd_2_0 in memory on 127.0.0.1:51732 (size: 13.1 MB, free: 889.0 MB)
25/07/25 09:42:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1914 bytes result sent to driver
25/07/25 09:42:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6094 ms on localhost (executor driver) (2/4)
25/07/25 09:42:59 INFO MemoryStore: Block rdd_2_1 stored as values in memory (estimated size 13.1 MB, free 875.6 MB)
25/07/25 09:42:59 INFO BlockManagerInfo: Added rdd_2_1 in memory on 127.0.0.1:51732 (size: 13.1 MB, free: 875.9 MB)
25/07/25 09:42:59 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1914 bytes result sent to driver
25/07/25 09:42:59 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 6126 ms on localhost (executor driver) (3/4)
25/07/25 09:42:59 INFO MemoryStore: Block rdd_2_2 stored as values in memory (estimated size 13.0 MB, free 862.6 MB)
25/07/25 09:42:59 INFO BlockManagerInfo: Added rdd_2_2 in memory on 127.0.0.1:51732 (size: 13.0 MB, free: 862.9 MB)
25/07/25 09:42:59 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1914 bytes result sent to driver
25/07/25 09:42:59 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 6157 ms on localhost (executor driver) (4/4)
25/07/25 09:42:59 INFO DAGScheduler: ShuffleMapStage 0 (sql at <unknown>:0) finished in 6,313 s
25/07/25 09:42:59 INFO DAGScheduler: looking for newly runnable stages
25/07/25 09:42:59 INFO DAGScheduler: running: Set()
25/07/25 09:42:59 INFO DAGScheduler: waiting: Set(ResultStage 1)
25/07/25 09:42:59 INFO DAGScheduler: failed: Set()
25/07/25 09:42:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/07/25 09:42:59 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at sql at <unknown>:0), which has no missing parents
25/07/25 09:42:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.1 KB, free 862.6 MB)
25/07/25 09:42:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.8 KB, free 862.6 MB)
25/07/25 09:42:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:51732 (size: 3.8 KB, free: 862.9 MB)
25/07/25 09:42:59 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
25/07/25 09:42:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/07/25 09:42:59 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
25/07/25 09:42:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 7767 bytes)
25/07/25 09:42:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 4)
25/07/25 09:42:59 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks
25/07/25 09:42:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
25/07/25 09:42:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 4). 1696 bytes result sent to driver
25/07/25 09:42:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 79 ms on localhost (executor driver) (1/1)
25/07/25 09:42:59 INFO DAGScheduler: ResultStage 1 (sql at <unknown>:0) finished in 0,109 s
25/07/25 09:42:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/07/25 09:42:59 INFO DAGScheduler: Job 0 finished: sql at <unknown>:0, took 6,524625 s
25/07/25 09:42:59 INFO CodeGenerator: Code generated in 11.3013 ms
25/07/25 09:42:59 INFO SparkContext: Starting job: collect at utils.scala:24
25/07/25 09:42:59 INFO DAGScheduler: Registering RDD 15 (collect at utils.scala:24)
25/07/25 09:42:59 INFO DAGScheduler: Got job 1 (collect at utils.scala:24) with 1 output partitions
25/07/25 09:42:59 INFO DAGScheduler: Final stage: ResultStage 3 (collect at utils.scala:24)
25/07/25 09:42:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
25/07/25 09:42:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
25/07/25 09:42:59 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[15] at collect at utils.scala:24), which has no missing parents
25/07/25 09:42:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.3 KB, free 862.5 MB)
25/07/25 09:42:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.5 KB, free 862.5 MB)
25/07/25 09:42:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:51732 (size: 11.5 KB, free: 862.9 MB)
25/07/25 09:43:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
25/07/25 09:43:00 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[15] at collect at utils.scala:24) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
25/07/25 09:43:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks
25/07/25 09:43:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8340 bytes)
25/07/25 09:43:00 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6, localhost, executor driver, partition 1, PROCESS_LOCAL, 8340 bytes)
25/07/25 09:43:00 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7, localhost, executor driver, partition 2, PROCESS_LOCAL, 8340 bytes)
25/07/25 09:43:00 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 8, localhost, executor driver, partition 3, PROCESS_LOCAL, 8340 bytes)
25/07/25 09:43:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)
25/07/25 09:43:00 INFO Executor: Running task 3.0 in stage 2.0 (TID 8)
25/07/25 09:43:00 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)
25/07/25 09:43:00 INFO BlockManager: Found block rdd_2_1 locally
25/07/25 09:43:00 INFO Executor: Running task 2.0 in stage 2.0 (TID 7)
25/07/25 09:43:00 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 1828 bytes result sent to driver
25/07/25 09:43:00 INFO BlockManager: Found block rdd_2_2 locally
25/07/25 09:43:00 INFO BlockManager: Found block rdd_2_3 locally
25/07/25 09:43:00 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 47 ms on localhost (executor driver) (1/4)
25/07/25 09:43:00 INFO BlockManager: Found block rdd_2_0 locally
25/07/25 09:43:00 INFO Executor: Finished task 3.0 in stage 2.0 (TID 8). 1828 bytes result sent to driver
25/07/25 09:43:00 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 8) in 47 ms on localhost (executor driver) (2/4)
25/07/25 09:43:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 1828 bytes result sent to driver
25/07/25 09:43:00 INFO Executor: Finished task 2.0 in stage 2.0 (TID 7). 1828 bytes result sent to driver
25/07/25 09:43:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 63 ms on localhost (executor driver) (3/4)
25/07/25 09:43:00 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 63 ms on localhost (executor driver) (4/4)
25/07/25 09:43:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/07/25 09:43:00 INFO DAGScheduler: ShuffleMapStage 2 (collect at utils.scala:24) finished in 0,079 s
25/07/25 09:43:00 INFO DAGScheduler: looking for newly runnable stages
25/07/25 09:43:00 INFO DAGScheduler: running: Set()
25/07/25 09:43:00 INFO DAGScheduler: waiting: Set(ResultStage 3)
25/07/25 09:43:00 INFO DAGScheduler: failed: Set()
25/07/25 09:43:00 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at collect at utils.scala:24), which has no missing parents
25/07/25 09:43:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.1 KB, free 862.5 MB)
25/07/25 09:43:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 862.5 MB)
25/07/25 09:43:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:51732 (size: 3.8 KB, free: 862.9 MB)
25/07/25 09:43:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
25/07/25 09:43:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at collect at utils.scala:24) (first 15 tasks are for partitions Vector(0))
25/07/25 09:43:00 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
25/07/25 09:43:00 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, ANY, 7767 bytes)
25/07/25 09:43:00 INFO Executor: Running task 0.0 in stage 3.0 (TID 9)
25/07/25 09:43:00 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks
25/07/25 09:43:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
25/07/25 09:43:00 INFO Executor: Finished task 0.0 in stage 3.0 (TID 9). 1696 bytes result sent to driver
25/07/25 09:43:00 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 16 ms on localhost (executor driver) (1/1)
25/07/25 09:43:00 INFO DAGScheduler: ResultStage 3 (collect at utils.scala:24) finished in 0,031 s
25/07/25 09:43:00 INFO DAGScheduler: Job 1 finished: collect at utils.scala:24, took 0,117390 s
25/07/25 09:43:00 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
25/07/25 09:43:00 INFO CodeGenerator: Code generated in 13.2186 ms
25/07/25 09:43:00 INFO CodeGenerator: Code generated in 12.3924 ms
25/07/25 09:43:00 INFO SparkContext: Starting job: collect at utils.scala:24
25/07/25 09:43:00 INFO DAGScheduler: Got job 2 (collect at utils.scala:24) with 1 output partitions
25/07/25 09:43:00 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:24)
25/07/25 09:43:00 INFO DAGScheduler: Parents of final stage: List()
25/07/25 09:43:00 INFO DAGScheduler: Missing parents: List()
25/07/25 09:43:00 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[22] at collect at utils.scala:24), which has no missing parents
25/07/25 09:43:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.5 KB, free 862.5 MB)
25/07/25 09:43:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.3 KB, free 862.5 MB)
25/07/25 09:43:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:51732 (size: 3.3 KB, free: 862.9 MB)
25/07/25 09:43:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
25/07/25 09:43:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[22] at collect at utils.scala:24) (first 15 tasks are for partitions Vector(0))
25/07/25 09:43:00 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
25/07/25 09:43:00 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8080 bytes)
25/07/25 09:43:00 INFO Executor: Running task 0.0 in stage 4.0 (TID 10)
25/07/25 09:43:00 INFO CodeGenerator: Code generated in 6.6739 ms
25/07/25 09:43:00 INFO Executor: Finished task 0.0 in stage 4.0 (TID 10). 1200 bytes result sent to driver
25/07/25 09:43:00 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 10) in 32 ms on localhost (executor driver) (1/1)
25/07/25 09:43:00 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
25/07/25 09:43:00 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:24) finished in 0,032 s
25/07/25 09:43:00 INFO DAGScheduler: Job 2 finished: collect at utils.scala:24, took 0,035791 s
25/07/25 09:43:02 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:43:02 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:43:02 INFO HiveMetaStore: 0: get_database: default
25/07/25 09:43:02 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_database: default	
25/07/25 09:43:02 INFO HiveMetaStore: 0: get_tables: db=default pat=*
25/07/25 09:43:02 INFO audit: ugi=Usuario	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
25/07/25 09:43:02 INFO CodeGenerator: Code generated in 8.8991 ms
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 34
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 91
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 96
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 127
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 114
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 79
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 120
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 142
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 123
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 169
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 106
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 148
25/07/25 10:11:21 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:51732 in memory (size: 11.5 KB, free: 862.9 MB)
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 122
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 151
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 81
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 117
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 124
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 119
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 125
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 77
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 155
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 157
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 109
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 138
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 160
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 154
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 111
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 133
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 149
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 89
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 159
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 87
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 83
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 163
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 140
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 135
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 143
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 158
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 165
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 94
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 121
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 167
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 134
25/07/25 10:11:21 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:51732 in memory (size: 3.8 KB, free: 862.9 MB)
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 129
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 90
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 95
25/07/25 10:11:21 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:51732 in memory (size: 3.8 KB, free: 862.9 MB)
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 150
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 107
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 146
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 139
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 166
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 80
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 136
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 100
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 145
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 101
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 141
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 156
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 128
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 130
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 144
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 85
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 131
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 137
25/07/25 10:11:21 INFO ContextCleaner: Cleaned shuffle 1
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 153
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 116
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 84
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 88
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 93
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 168
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 161
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 118
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 76
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 164
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 102
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 92
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 104
25/07/25 10:11:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:51732 in memory (size: 3.3 KB, free: 862.9 MB)
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 78
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 86
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 113
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 108
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 126
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 98
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 115
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 112
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 82
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 132
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 97
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 105
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 110
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 147
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 162
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 99
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 103
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 152
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 14
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 59
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 54
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 58
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 20
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 8
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 40
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 18
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 31
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 55
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 17
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 19
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 64
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 37
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 7
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 71
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 43
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 32
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 52
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 28
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 33
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 66
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 26
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 47
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 29
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 42
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 74
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 57
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 56
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 16
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 53
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 62
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 27
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 6
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 48
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 9
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 65
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 60
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 13
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 5
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 67
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 72
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 69
25/07/25 10:11:21 INFO ContextCleaner: Cleaned shuffle 0
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 45
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 50
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 41
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 70
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 39
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 68
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 11
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 51
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 35
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 46
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 61
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 49
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 15
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 44
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 75
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 12
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 36
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 73
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 63
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 30
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 10
25/07/25 10:11:21 INFO ContextCleaner: Cleaned accumulator 38
25/07/25 10:11:21 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:51732 in memory (size: 11.5 KB, free: 862.9 MB)
25/07/25 12:16:42 INFO SparkContext: Invoking stop() from shutdown hook
25/07/25 12:16:42 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
25/07/25 12:16:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/07/25 12:16:42 INFO MemoryStore: MemoryStore cleared
25/07/25 12:16:42 INFO BlockManager: BlockManager stopped
25/07/25 12:16:42 INFO BlockManagerMaster: BlockManagerMaster stopped
25/07/25 12:16:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/07/25 12:16:42 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\Usuario\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e0d71c16-2d49-4ccc-b1ff-f55871ba6a18\userFiles-118793d3-6e19-4d6a-bc0a-c3d246a45ff4
java.io.IOException: Failed to delete: C:\Users\Usuario\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e0d71c16-2d49-4ccc-b1ff-f55871ba6a18\userFiles-118793d3-6e19-4d6a-bc0a-c3d246a45ff4\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1974)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1973)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
25/07/25 12:16:42 INFO SparkContext: Successfully stopped SparkContext
25/07/25 12:16:42 INFO ShutdownHookManager: Shutdown hook called
25/07/25 12:16:42 INFO ShutdownHookManager: Deleting directory C:\Users\Usuario\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e0d71c16-2d49-4ccc-b1ff-f55871ba6a18
25/07/25 12:16:42 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Usuario\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e0d71c16-2d49-4ccc-b1ff-f55871ba6a18
java.io.IOException: Failed to delete: C:\Users\Usuario\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e0d71c16-2d49-4ccc-b1ff-f55871ba6a18\userFiles-118793d3-6e19-4d6a-bc0a-c3d246a45ff4\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
25/07/25 12:16:42 INFO ShutdownHookManager: Deleting directory C:\Users\Usuario\AppData\Local\Temp\spark-28ac89b7-ee91-4ac9-bfa4-a6a569084e42
25/07/25 12:16:42 INFO ShutdownHookManager: Deleting directory C:\Users\Usuario\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e0d71c16-2d49-4ccc-b1ff-f55871ba6a18\userFiles-118793d3-6e19-4d6a-bc0a-c3d246a45ff4
25/07/25 12:16:42 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Usuario\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e0d71c16-2d49-4ccc-b1ff-f55871ba6a18\userFiles-118793d3-6e19-4d6a-bc0a-c3d246a45ff4
java.io.IOException: Failed to delete: C:\Users\Usuario\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e0d71c16-2d49-4ccc-b1ff-f55871ba6a18\userFiles-118793d3-6e19-4d6a-bc0a-c3d246a45ff4\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
